{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TF Lite model with RETVec\n",
    "\n",
    "Please note that using RETVec with TF Lite requires `tensorflow_text>=2.13` and `tensorflow>=2.13`. You can upgrade your TensorFlow following the instructions [here](https://www.tensorflow.org/install/pip).\n",
    "\n",
    "This notebook shows how to create, save, and run a TF Lite compatible model which uses the RETVec tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 23:59:09.641898: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 23:59:09.701389: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# installing retvec if needed\n",
    "try:\n",
    "    import retvec\n",
    "except ImportError:\n",
    "    !pip install retvec\n",
    "\n",
    "try:\n",
    "    import tensorflow_text\n",
    "except ImportError:\n",
    "    !pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # silence TF INFO messages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import the RETVec tokenizer layer\n",
    "from retvec.tf import RETVecTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only important change to make for RETVec is to set `use_native_tf_ops=True`. This will make the layer use `tensorflow_text.utf8_binarize` which is supported natively by TF Lite.\n",
    "\n",
    "Note that in this example we use a simple dense model to show conversion, since LSTM layers [require additional effort to convert to TF Lite](https://www.tensorflow.org/lite/models/convert/rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 23:59:14.286108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 643 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:18:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.288539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 77617 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2a:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.290486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 77617 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:3a:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.292434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 77617 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5d:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.294351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 77617 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:9a:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.296319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 77617 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:ab:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.298450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 77617 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:ba:00.0, compute capability: 9.0\n",
      "2023-10-11 23:59:14.300441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 77617 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:db:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " ret_vec_tokenizer (RETVecT  (None, 128, 256)          230144    \n",
      " okenizer)                                                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128, 256)          65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128, 64)           16448     \n",
      "                                                                 \n",
      " output (Dense)              (None, 128, 4)            260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 312644 (1.19 MB)\n",
      "Trainable params: 82500 (322.27 KB)\n",
      "Non-trainable params: 230144 (899.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using strings directly requires to put a shape of (1,) and dtype tf.string\n",
    "inputs = layers.Input(shape=(1, ), name=\"input\", dtype=tf.string)\n",
    "\n",
    "# add RETVec tokenizer layer with `use_native_tf_ops`\n",
    "x = RETVecTokenizer(model='retvec-v1', use_native_tf_ops=True)(inputs)\n",
    "\n",
    "# build the rest of the model as usual\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(4, activation='sigmoid', name=\"output\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <retvec.tf.layers.binarizer.RETVecBinarizer object at 0x7f54c1db7e80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f529c150340>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.spatial_dropout1d.SpatialDropout1D object at 0x7f529c1506d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./demo_models/tf_lite_retvec/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./demo_models/tf_lite_retvec/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "save_path = \"./demo_models/tf_lite_retvec\"\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model and run inference in TF Lite\n",
    "\n",
    "We can now convert the model to a TF Lite model following the [instructions](https://www.tensorflow.org/lite/models/convert). For more information on how to use TensorFlow Lite, please see the [guide](https://www.tensorflow.org/lite/guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 23:59:17.933195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-10-11 23:59:17.933219: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-10-11 23:59:17.933697: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./demo_models/tf_lite_retvec\n",
      "2023-10-11 23:59:17.937058: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-10-11 23:59:17.937071: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./demo_models/tf_lite_retvec\n",
      "2023-10-11 23:59:17.956432: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-10-11 23:59:17.959221: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-10-11 23:59:18.023007: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: ./demo_models/tf_lite_retvec\n",
      "2023-10-11 23:59:18.049777: I tensorflow/cc/saved_model/loader.cc:334] SavedModel load for tags { serve }; Status: success: OK. Took 116080 microseconds.\n",
      "2023-10-11 23:59:18.099129: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-11 23:59:18.281502: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2084] The following operation(s) need TFLite custom op implementation(s):\n",
      "Custom ops: RaggedTensorToTensor, TFText>Utf8Binarize, TFText>WhitespaceTokenizeWithOffsetsV2\n",
      "Details:\n",
      "\ttf.RaggedTensorToTensor(tensor<3xi32>, tensor<?x384xf32>, tensor<f32>, tensor<?xi64>) -> (tensor<?x128x384xf32>) : {T = f32, Tindex = i64, Tshape = i32, device = \"\", num_row_partition_tensors = 1 : i64, row_partition_types = [\"ROW_SPLITS\"]}\n",
      "\ttf.TFText>Utf8Binarize(tensor<?x!tf_type.string>) -> (tensor<?x384xf32>) : {bits_per_char = 24 : i64, device = \"\", replacement_char = 65533 : i64, word_length = 16 : i64}\n",
      "\ttf.TFText>WhitespaceTokenizeWithOffsetsV2(tensor<?x!tf_type.string>, tensor<!tf_type.string>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi32>, tensor<?xi32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_custom\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(save_path) # path to the SavedModel directory\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "]\n",
    "converter.allow_custom_ops = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.lite.python import interpreter\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "# create TF lite interpreter with TF Text ops registered\n",
    "interp = interpreter.InterpreterWithCustomOps(\n",
    "    model_content=tflite_model,\n",
    "    custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
    "interp.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite result =  [[[0.44056153 0.64904165 0.6789909  0.48179772]\n",
      "  [0.57152873 0.42630616 0.56769997 0.53362656]\n",
      "  [0.49272767 0.50642216 0.5604572  0.45996392]\n",
      "  [0.5502532  0.3399313  0.6912261  0.6570215 ]\n",
      "  [0.39991876 0.6124565  0.6016915  0.5008456 ]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]\n",
      "  [0.45399478 0.62730265 0.4977278  0.51457846]]]\n"
     ]
    }
   ],
   "source": [
    "# run inference with our model\n",
    "input_data = np.array(['This is an example text'])\n",
    "\n",
    "tokenize = interp.get_signature_runner('serving_default')\n",
    "output = tokenize(input=input_data)\n",
    "print('TensorFlow Lite result = ', output['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
